{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')\n",
    "df.isnull().sum()\n",
    "df.isnull().sum() > 0\n",
    "\n",
    "# Mostrar columnas con nº datos nulos\n",
    "null_cols = df.isnull().sum()\n",
    "null_cols[null_cols > 0]\n",
    "null_cols[null_cols > 0] / len(data) * 100 # Porcentaje de datos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar el final que esta metido un poco a cholon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra -> Rehacer con datos.\n",
    "# Se están comparando columnas contra price (ej diamantes)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=5, nrows=1, figsize=(20, 5))\n",
    "ax1 = axes[0]\n",
    "ax2 = axes[1]\n",
    "ax3 = axes[2]\n",
    "ax4 = axes[3]\n",
    "ax5 = axes[4]\n",
    "\n",
    "for ax, column in [[ax1, 'carat'], [ax2, 'depth'], [ax3, 'x'], [ax4, 'y'], [ax5, 'z']]:\n",
    "    ax.scatter(train[column], train['price'])\n",
    "    ax.set_title(f'{column} vs price')\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Price')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".reset_index() despues de cada modificacion en el df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasformación del df \n",
    "# Detección de outlayers y gestión (borrado o mantener)\n",
    "# Numerical and categorical features\n",
    "\n",
    "NUM_FEATS = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
    "CAT_FEATS = ['cut', 'color', 'clarity']\n",
    "FEATS = NUM_FEATS + CAT_FEATS  #FEATS es features\n",
    "TARGET = 'price'\n",
    "\n",
    "# Imputar valores nulos\n",
    "# Estandarizar\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), \n",
    "                                      ('scaler', StandardScaler())])  \n",
    "\n",
    "categorical_transformer = Pipeline(steps = [('imputer', SimpleImputer(strategy='constant', \n",
    "                                                                      fill_value='missing')),\n",
    "                                             ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, NUM_FEATS), \n",
    "                                               ('cat', categorical_transformer, CAT_FEATS)])\n",
    "\n",
    "pd.DataFrame(preprocessor.fit_transform(df)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar train test\n",
    "\n",
    "df_train, df_test = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo ML\n",
    "\n",
    "model= Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('regressor', RandomForestRegressor())])\n",
    "\n",
    "# Primera prueba (train test)\n",
    "\n",
    "y_train_real = df_train[TARGET]\n",
    "y_train_pred = model.predict(df_train[FEATS])\n",
    "mean_squared_error(y_pred=y_train_pred, y_true=y_train_real, squared=False)\n",
    "r2_score(y_train_real, y_train_pred)\n",
    "\n",
    "y_test_real = df_test[TARGET]\n",
    "y_test_pred = model.predict(df_test[FEATS])\n",
    "mean_squared_error(y_pred=y_test_pred, y_true=y_test_real, squared=False)\n",
    "r2_score(y_test_real, y_test_pred)\n",
    "\n",
    "# Validar con cross validation\n",
    "\n",
    "scores = cross_val_score(model,\n",
    "                        df[FEATS],\n",
    "                        df[TARGET],\n",
    "                        scoring='neg_root_mean_squared_error',\n",
    "                        cv=5, n_jobs=-1)\n",
    "\n",
    "np.mean(scores)\n",
    "\n",
    "# Búsqueda de hpyerparámetros\n",
    "\n",
    "param_grid = {\n",
    "    'preprocessor__num__imputer__strategy':['mean', 'median'],\n",
    "    'regressor__n_estimators': [16, 32, 64, 128, 256, 512],\n",
    "    'regressor__max_depth': [2, 4, 8, 16],\n",
    "    }\n",
    "\n",
    "grid_search = RandomizedSearchCV(model, \n",
    "                                 param_grid, \n",
    "                                 cv=8, \n",
    "                                 verbose=10, \n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                n_jobs=-1,\n",
    "                                n_iter=30\n",
    "                                )\n",
    "\n",
    "grid_search.fit(df[TARGET], df[FEATS])\n",
    "\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "\n",
    "# Validar con cross validation\n",
    "\n",
    "scores = cross_val_score(model,\n",
    "                        df_test[FEATS],\n",
    "                        df_test[TARGET],\n",
    "                        scoring='r2', # r2 bien ahi? o mejor poner 'neg_root_mean_squared_error' ¿?\n",
    "                        cv=5, n_jobs=-1)\n",
    "\n",
    "np.mean(scores)\n",
    "\n",
    "# Predecir\n",
    "\n",
    "y_pred = grid_search.predict(df_predict[FEATS]) #df_predict es el df que nos dan para predecir\n",
    "\n",
    "# Juntar en df con id\n",
    "\n",
    "submission_df = pd.DataFrame({'id': df_predict['id'], 'price': y_pred})\n",
    "submission_df.head()\n",
    "submission_df.describe() # Analizar por si hay valores atípicos (outliers)\n",
    "submission_df.price.clip(0, 20000, inplace=True) # De los valores obtenidos en describe, con clip limitamos el valor máximo y el mínimo\n",
    "\n",
    "# Exportar a .csv\n",
    "submission_df.to_csv('../prueba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar conlightgbm\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                       ('regressor', LGBMRegressor(boosting_type='gbdt', \n",
    "                       bagging_freq=1, \n",
    "                       bagging_fraction = 0.9, \n",
    "                       n_estimators=100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA a cholon -> Revisar el notebook \"analisis_exploratorio.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, nrows=1, figsize=(30, 7))\n",
    "corr_methods = ['pearson', 'kendall', 'spearman']\n",
    "\n",
    "for i in range(len(corr_methods)):\n",
    "    sns.heatmap(df.corr(method=corr_methods[i]), annot=True, fmt='.2f', ax=ax[i]);\n",
    "    ax[i].set_title(f'{corr_methods[i].upper()} Correlation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack_env]",
   "language": "python",
   "name": "conda-env-ironhack_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
